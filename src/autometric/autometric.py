# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/library/string-geodesics.ipynb.

# %% auto 0
__all__ = ['compute_jacobian_function', 'pullback_metric', 'StringGeodesic']

# %% ../../nbs/library/string-geodesics.ipynb 4
from torch.autograd import grad
def compute_jacobian_function(f, x, create_graph=False, retain_graph=True):
    """
    Compute the Jacobian of the decoder wrt a batch of points in the latent space using an efficient broadcasting approach.
    :param model: The VAE model.
    :param z_batch: A batch of points in the latent space (tensor).
    :return: A batch of Jacobian matrices.
    """
    # z_batch = z_batch.clone().detach().requires_grad_(True)
    x = x.clone()
    x.requires_grad_(True)
    # model.no_grad()
    output = torch.vmap(f)(x)
    batch_size, output_dim, latent_dim = *output.shape, x.shape[-1]

    # Use autograd's grad function to get gradients for each output dimension
    jacobian = torch.zeros(batch_size, output_dim, latent_dim).to(x.device)
    for i in range(output_dim):
        grad_outputs = torch.zeros(batch_size, output_dim).to(x.device)
        grad_outputs[:, i] = 1.0
        gradients = grad(outputs=output, inputs=x, grad_outputs=grad_outputs, create_graph=create_graph, retain_graph=retain_graph, only_inputs=True)[0]
        jacobian[:, i, :] = gradients
    return jacobian

def pullback_metric(fcn, x, create_graph=False, retain_graph=True):
    jac = compute_jacobian_function(fcn, x, create_graph, retain_graph)
    metric = torch.einsum('Nki,Nkj->Nij', jac, jac)
    return metric

# %% ../../nbs/library/string-geodesics.ipynb 5
import torch
from torch import nn
import lightning as pl

class StringGeodesic(pl.LightningModule):
    def __init__(self, 
                 immersion, # a metric object.
                 start, # starting point
                 end, # ending point
                 dim: int = 2, # dimension of the space in which the metric lives.
                 num_beads: int = 1000,
                 flexibility: int = 10,
                 step_size = 0.1,
                 learning_rate = 1e-5, 
                 device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                 ):
        super().__init__()
        
        self.dim = dim
        self.num_beads = num_beads
        self.immersion = immersion
        self.flexibility = flexibility
        self.step_size = step_size
        self.lr = learning_rate
        
        self.beads = torch.vstack(
            [torch.lerp(start, end, t) for t in torch.linspace(0,1,steps=num_beads)]
        ).float().to(device)
        self.metric_per_bead = self.get_metric(self.beads).float()
        self.distances = self.progressive_lengths(self.beads, self.metric_per_bead)

        # Create the network layers
        self.Force = nn.Sequential(
            nn.Linear(dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, dim),
        )
    
    def get_metric(self, base_points):
        return torch.concatenate([torch.eye(self.dim).to(base_points.device)[None, :, :] for i in range(len(base_points))], dim=0)
        # return pullback_metric(
        #     self.immersion,
        #     base_points,
        #     )
    
    def inner_product(self, vec, metric):
        return vec @ metric @ vec
        
    def progressive_lengths(self, beads, metric_per_bead):
        # Computes length between each consecutive bead
        lengths = torch.zeros(self.num_beads, device = beads.device)
        for i in range(1, len(beads)):
            lengths[i]  = lengths[i-1] + torch.sqrt(self.inner_product(beads[i] -  beads[i-1], metric_per_bead[i]))
            
        print(f"{beads.device=} {metric_per_bead.device=} {lengths.device=}")
        return lengths
    
    def apply_force(self, beads, forces, lengths):
        # applies the force given to each bead, modulating by lengths
        distances_from_end = torch.sqrt(lengths * (lengths[-1] - lengths)) 
        force_modulator = torch.sigmoid(self.flexibility*distances_from_end)*2 - 1
        print(f'{force_modulator.device=} {distances_from_end.device=} {beads.device=}')
        fm = (self.step_size*force_modulator[:,None])
        beads_after_wind = fm*forces + beads
        return beads_after_wind
        
    def step(self, batch, batch_idx):
        # calculate force assigned to each bead
        force = self.Force(self.beads)
        # apply the force to get new beads
        self.beads = self.apply_force(self.beads, force, self.distances)
        self.metric_per_bead = self.get_metric(self.beads).float()
        self.distances = self.progressive_lengths(self.beads, self.metric_per_bead)
        return self.distances[-1]
    
    def training_step(self, batch, batch_idx):
        loss = self.step(batch, batch_idx)
        self.log('train_loss', loss, prog_bar=True, on_epoch=True)
        return loss
    
    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
        return optimizer
