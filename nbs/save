{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8482f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp n1d_embedding_analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import deepdish\n",
    "\n",
    "import os\n",
    "os.environ[\"GEOMSTATS_BACKEND\"] = \"pytorch\"\n",
    "\n",
    "# models\n",
    "import torch\n",
    "from autometric.autoencoders import DistanceMatchingAutoencoder\n",
    "from autometric.datasets import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09145b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from autometric.metrics import PullbackMetric\n",
    "import numpy as np\n",
    "import torch\n",
    "def determinants_of_encoder_pullback(model, dataloader):\n",
    "    # returns the determinants of the metric matrices for each point in the dataset\n",
    "    Metric = PullbackMetric(model.input_dim, model.encoder)\n",
    "    Gs = Metric.metric_matrix(dataloader.dataset.pointcloud).detach().cpu().numpy()\n",
    "    dets = [np.linalg.det(G) for G in Gs]\n",
    "    return np.array(dets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd31bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from autometric.metrics import PullbackMetric\n",
    "import numpy as np\n",
    "import torch\n",
    "def trace_of_encoder_pullback(model, dataloader):\n",
    "    # returns the determinants of the metric matrices for each point in the dataset\n",
    "    Metric = PullbackMetric(model.input_dim, model.encoder)\n",
    "    Gs = Metric.metric_matrix(dataloader.dataset.pointcloud).detach().cpu().numpy()\n",
    "    dets = [np.sum(np.linalg.eigvals(G)) for G in Gs]\n",
    "    return np.array(dets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22074db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from autometric.metrics import PullbackMetric\n",
    "import numpy as np\n",
    "import torch\n",
    "def rank_of_encoder_pullback(model, dataloader, eps=1e-10):\n",
    "    # returns the determinants of the metric matrices for each point in the dataset\n",
    "    Metric = PullbackMetric(model.input_dim, model.encoder)\n",
    "    Gs = Metric.metric_matrix(dataloader.dataset.pointcloud).detach().cpu().numpy()\n",
    "    ranks = [np.sum((np.linalg.eigvals(G)>eps).astype(int)) for G in Gs]\n",
    "    return np.array(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6d3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from autometric.metrics import PullbackMetric\n",
    "import numpy as np\n",
    "import torch\n",
    "def spectral_entropy_of_matrix(A):\n",
    "    # returns the spectral entropy of a matrix\n",
    "    # A is a numpy array\n",
    "    eigvals = np.linalg.eigvals(A)\n",
    "    eigvals = eigvals[eigvals > 0]\n",
    "    eigvals /= eigvals.sum()\n",
    "    return -np.sum(eigvals * np.log(eigvals))\n",
    "\n",
    "def spectral_entropy_of_encoder_pullback(model, dataloader):\n",
    "    # returns the determinants of the metric matrices for each point in the dataset\n",
    "    Metric = PullbackMetric(model.input_dim, model.encoder)\n",
    "    Gs = Metric.metric_matrix(dataloader.dataset.pointcloud).detach().cpu().numpy()\n",
    "    entropies = [spectral_entropy_of_matrix(G) for G in Gs]\n",
    "    return np.array(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15bfdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def evals_of_encoder_pullback(model, dataloader):\n",
    "    # returns the determinants of the metric matrices for each point in the dataset\n",
    "    Metric = PullbackMetric(model.input_dim, model.encoder)\n",
    "    Gs = Metric.metric_matrix(dataloader.dataset.pointcloud).detach().cpu().numpy()\n",
    "    e = [np.sort(np.linalg.eigvals(G))[::-1] for G in Gs]\n",
    "    return np.vstack(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e314301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def smallest_eigenvector(matrix):\n",
    "    \"\"\"\n",
    "    Find the eigenvector associated with the smallest eigenvalue of a square matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix (np.array): A square numpy array representing a matrix.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The eigenvector associated with the smallest eigenvalue.\n",
    "    \"\"\"\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "\n",
    "    # Find the index of the smallest eigenvalue\n",
    "    min_index = np.argmin(eigenvalues)\n",
    "\n",
    "    # Return the corresponding eigenvector\n",
    "    return eigenvectors[:, min_index]\n",
    "def normal_vectors_of_encoder_pullback(model, dataloader):\n",
    "    # returns the determinants of the metric matrices for each point in the dataset\n",
    "    Metric = PullbackMetric(model.input_dim, model.encoder)\n",
    "    Gs = Metric.metric_matrix(dataloader.dataset.pointcloud).detach().cpu().numpy()\n",
    "    e = [smallest_eigenvector(G) for G in Gs]\n",
    "    return np.vstack(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa21d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from autometric.utils import *\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def visualize_encoder_pullback_metrics(model, dataloader, title):\n",
    "    X = model.encoder(dataloader.dataset.pointcloud).cpu().detach().numpy()\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "    spectral_entropy = spectral_entropy_of_encoder_pullback(model,dataloader)\n",
    "    axs[0,0].scatter(X[:,0],X[:,1],c=spectral_entropy)\n",
    "    axs[0,0].set_title(\"Spectral Entropy\")\n",
    "\n",
    "    trace = trace_of_encoder_pullback(model,dataloader)\n",
    "    axs[0,1].scatter(X[:,0],X[:,1],c=trace)\n",
    "    axs[0,1].set_title(\"Trace\")\n",
    "\n",
    "    rank = rank_of_encoder_pullback(model,dataloader)\n",
    "    axs[0,2].scatter(X[:,0],X[:,1],c=rank)\n",
    "    axs[0,2].set_title(\"Rank\")\n",
    "    \n",
    "    evals = evals_of_encoder_pullback(model, dataloader)\n",
    "    for i in range(3):\n",
    "        axs[1,i].scatter(X[:,0],X[:,1],c=evals[:,i])\n",
    "        axs[1,i].set_title(f\"{printnum(i)} Eigenvalue\")\n",
    "    \n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e17272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from autometric.utils import *\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def visualize_encoder_pullback_metrics_in_ambient_space(model, dataloader, title):\n",
    "    X = model.encoder(dataloader.dataset.pointcloud).cpu().detach().numpy()\n",
    "    D = dataloader.dataset.pointcloud.cpu().detach().numpy()\n",
    "    figure = plt.figure()\n",
    "\n",
    "    ax = figure.add_subplot(231, projection='3d')\n",
    "    spectral_entropy = spectral_entropy_of_encoder_pullback(model,dataloader)\n",
    "    ax.scatter(D[:,0],D[:,1],D[:,2],c=spectral_entropy)\n",
    "    ax.set_title(\"Spectral Entropy\")\n",
    "\n",
    "    ax = figure.add_subplot(232, projection='3d')\n",
    "    trace = trace_of_encoder_pullback(model,dataloader)\n",
    "    ax.scatter(D[:,0],D[:,1],D[:,2],c=trace)\n",
    "    ax.set_title(\"Trace\")\n",
    "\n",
    "    ax = figure.add_subplot(233, projection='3d')\n",
    "    rank = rank_of_encoder_pullback(model,dataloader)\n",
    "    ax.scatter(D[:,0],D[:,1],D[:,2],c=rank)\n",
    "    ax.set_title(\"Rank\")\n",
    "    \n",
    "    evals = evals_of_encoder_pullback(model, dataloader)\n",
    "    for i in range(3):\n",
    "        ax = figure.add_subplot(230+i+4, projection='3d')\n",
    "        ax.scatter(D[:,0],D[:,1],D[:,2],c=evals[:,i])\n",
    "        ax.set_title(f\"{printnum(i)} Eigenvalue\")\n",
    "    \n",
    "    figure.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9af6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if not latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e13b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a pytorch lightning trainer and train DerrickTheAutoencoder on the sphere dataset\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "from autometric.autoencoders import DerrickTheAutoencoder\n",
    "import os\n",
    "from autometric.datasets import PointCloudDataset\n",
    "\n",
    "from diffusion_curvature.datasets import sphere, torus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a459c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a pytorch lightning trainer and train DerrickTheAutoencoder on the sphere dataset\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "from autometric.autoencoders import DerrickTheAutoencoder\n",
    "import os\n",
    "from autometric.datasets import PointcloudDataset\n",
    "\n",
    "from diffusion_curvature.datasets import sphere, torus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fed2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a pytorch lightning trainer and train DerrickTheAutoencoder on the sphere dataset\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "from autometric.autoencoders import DerrickTheAutoencoder\n",
    "import os\n",
    "from autometric.datasets import PointcloudDataset\n",
    "\n",
    "from diffusion_curvature.datasets import sphere, torus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ebe418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping('val_loss', patience=500)\n",
    "trainer = Trainer(max_epochs=1000, \n",
    "                #   accelerator='cuda',\n",
    "                  callbacks=[early_stopping])\n",
    "model = DerrickTheAutoencoder(input_dim=3, intrinsic_dimension=2)\n",
    "# check if model alreaady exists at ../data/sphere_encoder.pt - if so, load it\n",
    "if os.path.exists('../data/sphere_encoder.pt'):\n",
    "    model.load_state_dict(torch.load('../data/sphere_encoder.pt'))\n",
    "else:\n",
    "    trainer.fit(\n",
    "        model=model,\n",
    "        train_dataloaders=trainloader,\n",
    "        val_dataloaders=valloader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f799c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%notebook save\n",
    "!nbdev_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c602b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sphere, ks_sphere = sphere(n=10000)\n",
    "X_train = X_sphere[:8000]\n",
    "train_dataset = PointcloudDataset(X_train)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa6010d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f10eb930",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a19ea811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9af6e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06a901d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sphere, ks_sphere = sphere(n=10000)\n",
    "train_dataset = PointcloudDataset(X_sphere)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35381100",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd9676db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.util import get_coordinates\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee86110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2519daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.util import get_coordinates\n",
    "from autometric.geometry import LeviCevitaConnection, PullbackMetric, RiemannianManifold\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e45d6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.util import get_coordinates\n",
    "from autometric.geometry import *\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2675b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sphere, ks_sphere = sphere(n=10000)\n",
    "train_dataset = PointcloudDataset(X_sphere)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89fa216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sphere, ks_sphere = sphere(n=10000)\n",
    "train_dataset = PointcloudDataset(X_sphere)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69140eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sphere, ks_sphere = sphere(n=10000)\n",
    "train_dataset = PointcloudDataset(X_sphere)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e060ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18f43a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.util import get_coordinates\n",
    "from autometric.geometry import *\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fa8a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.util import get_coordinates\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.manifolds import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe87370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.util import get_coordinates\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76912caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6cd62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp n1d_embedding_analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import deepdish\n",
    "\n",
    "import os\n",
    "os.environ[\"GEOMSTATS_BACKEND\"] = \"pytorch\"\n",
    "\n",
    "# models\n",
    "import torch\n",
    "from autometric.autoencoders import DistanceMatchingAutoencoder\n",
    "from autometric.datasets import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ee0f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.util import get_coordinates\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a96002f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "770867ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.util import get_coordinates\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc, device=device)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebf75111",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df44a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2087a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0bdcd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.util import get_coordinates\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6050c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b987b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.util import *\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a0b9032",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9f45e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.util import *\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    print(vector_norms)\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d442b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad9893b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere scaling_factor=1/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b75860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bfe00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebb1ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60bbe67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4bd5d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/200, just_on_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e10eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/20, just_on_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8063c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/4, just_on_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c621b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/40, just_on_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9d8267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.util import *\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), closed=True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    print(vector_norms)\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f3572ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/40, just_on_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "31af8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from autometric.util import *\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), closed=True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "    print(vector_norms)\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "504b7d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from autometric.util import *\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    print(vector_patches.shape, vector_patches[0])\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), closed=True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e1d5a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from autometric.util import *\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    print(vector_patches.shape, vector_patches[0])\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), closed=True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d0697a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/40, just_on_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "195ac830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from autometric.util import *\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "    \n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        latent_activations = model.encode(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    print(vector_patches.shape, vector_patches[0])\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), closed=True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef921d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/40, just_on_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "edf17af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from autometric.util import *\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def get_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "    \n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        try:\n",
    "            latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "        except AttributeError:\n",
    "            latent_activations = model.encode(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    print(vector_patches.shape, vector_patches[0])\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), closed=True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69078b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/40, just_on_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e017cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/40, just_on_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3b9f34e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from autometric.util import *\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def plot_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        try:\n",
    "            latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "        except AttributeError:\n",
    "            latent_activations = model.encode(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), closed=True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "60386e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/40, just_on_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d3805412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from autometric.util import *\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "def plot_indicatrices(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    grid=\"convex_hull\",\n",
    "                    device=\"cpu\",\n",
    "                    num_steps=20,\n",
    "                    num_gon=50,\n",
    "                    output_path=None,\n",
    "                    writer=None,\n",
    "                    latent_activations=None,\n",
    "                    model_name=\"GeomReg\",\n",
    "                    dataset_name=\"MNIST\",\n",
    "                    labels=None,\n",
    "                    cmap=\"tab10\",\n",
    "                    just_on_data = False,\n",
    "                    scaling_factor = 1/4\n",
    "                    ):\n",
    "    if latent_activations is None:\n",
    "        pointcloud = dataloader.dataset.pointcloud\n",
    "        try:\n",
    "            latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "        except AttributeError:\n",
    "            latent_activations = model.encode(dataloader.dataset.pointcloud).cpu().detach()\n",
    "\n",
    "    generator = torch.Generator().manual_seed(0)\n",
    "    perm = torch.randperm(labels.shape[0], generator=generator)\n",
    "    \n",
    "    latent_activations = latent_activations[perm]\n",
    "    labels = labels[perm]\n",
    "\n",
    "    coordinates_on_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                          grid=\"on_data\",\n",
    "                                          num_steps=num_steps,\n",
    "                                          coords0=None,\n",
    "                                          dataset_name=dataset_name,\n",
    "                                          model_name=model_name).to(device)\n",
    "\n",
    "    coordinates_off_data = get_coordinates(torch.squeeze(latent_activations),\n",
    "                                           grid=\"off_data\",\n",
    "                                           num_steps=num_steps,\n",
    "                                           coords0=None,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           model_name=model_name).to(device)\n",
    "\n",
    "    if just_on_data:\n",
    "        coordinates = coordinates_on_data\n",
    "    else:\n",
    "        coordinates = torch.vstack([coordinates_on_data, coordinates_off_data])\n",
    "\n",
    "    # calculate grid step sizes\n",
    "    x_min = torch.min(latent_activations[:, 0]).item()\n",
    "    x_max = torch.max(latent_activations[:, 0]).item()\n",
    "    y_min = torch.min(latent_activations[:, 1]).item()\n",
    "    y_max = torch.max(latent_activations[:, 1]).item()\n",
    "\n",
    "    num_steps_x = num_steps\n",
    "    num_steps_y = int((y_max - y_min) / (x_max - x_min) * num_steps_x)\n",
    "\n",
    "    step_size_x = (x_max - x_min) / (num_steps_x)\n",
    "    step_size_y = (y_max - y_min) / (num_steps_y)\n",
    "    stepsize = min(step_size_x, step_size_y)\n",
    "\n",
    "    # # find initial coordinate\n",
    "    # if coords0 is not None:\n",
    "    #     coords0_index = None\n",
    "    #     for i, row in enumerate(coordinates.cpu()):\n",
    "    #         if torch.all(row.eq(coords0)):\n",
    "    #             coords0_index = i\n",
    "    coords0_index = 0\n",
    "    coords0 = latent_activations[0]\n",
    "\n",
    "    # initialize diffgeo objects\n",
    "    # TODO: Equip for arbitrary dimensions\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    # generate vector patches at grid points, normed in pullback metric\n",
    "    try:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates)\n",
    "    except RuntimeError:\n",
    "        vector_patches, norm_vectors = rm.generate_unit_vectors(num_gon, coordinates.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "    vector_patches = vector_patches.to(device)\n",
    "\n",
    "    vector_norms = torch.linalg.norm(vector_patches.reshape\n",
    "                                     (-1, 2), dim=1)\n",
    "    max_vector_norm = torch.min(vector_norms[torch.nonzero(vector_norms)])\n",
    "\n",
    "    normed_vector_patches = vector_patches / max_vector_norm * stepsize * scaling_factor  # / 3\n",
    "    anchored_vector_patches = coordinates.unsqueeze(1).expand(*normed_vector_patches.shape) + normed_vector_patches\n",
    "\n",
    "    # create polygons\n",
    "    polygons = [Polygon(tuple(vector.tolist()), closed=True) for vector in anchored_vector_patches]\n",
    "\n",
    "    polygons_on_data = polygons[:coordinates_on_data.shape[0]]\n",
    "    polygons_off_data = polygons[coordinates_on_data.shape[0]:]\n",
    "\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0 = polygons.pop(coords0_index)\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    \n",
    "    latent_activations = latent_activations.detach().cpu()\n",
    "\n",
    "    # plot blobs\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0.)\n",
    "    plt.margins(0.01, 0.01)\n",
    "\n",
    "    ax.scatter(latent_activations[:, 0],\n",
    "               latent_activations[:, 1],\n",
    "               c=labels,\n",
    "               cmap=cmap,\n",
    "               **get_sc_kwargs())\n",
    "\n",
    "    p_on_data = PatchCollection(polygons_on_data)\n",
    "    p_off_data = PatchCollection(polygons_off_data)\n",
    "    # p2 = PatchCollection(polygons2)\n",
    "\n",
    "    # p_off_data.set_edgecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "    # if model_name == \"Vanilla\" and dataset_name == \"Zilionis\":\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.0])\n",
    "    # else:\n",
    "    #    p_off_data.set_facecolor([0 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "\n",
    "    p_on_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "    p_off_data.set_color([0 / 255, 0 / 255, 0 / 255, 0.3])\n",
    "\n",
    "    if coords0 is not None:\n",
    "        polygon0.set_color([255 / 255, 0 / 255, 0 / 255, 0.2])\n",
    "        ax.add_patch(polygon0)\n",
    "\n",
    "    ax.add_collection(p_off_data)\n",
    "    ax.add_collection(p_on_data)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    # fig.suptitle(f\"Indicatrices\")\n",
    "\n",
    "    if output_path is not None:\n",
    "        plt.savefig(output_path, **get_saving_kwargs())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.add_figure(\"indicatrix\", fig)\n",
    "\n",
    "        # clean up tensorboard writer\n",
    "        writer.flush()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd26fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indicatrices(model, trainloader, labels = ks_sphere, scaling_factor=1/40, just_on_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6afea0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def indicatrix_volume_variance_metric(\n",
    "    model,\n",
    "    dataloader,\n",
    "):\n",
    "    pointcloud = dataloader.dataset.pointcloud\n",
    "    try:\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "    except AttributeError:\n",
    "        latent_activations = model.encode(dataloader.dataset.pointcloud).cpu().detach()\n",
    "    # set up manifold\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "    # calculate the logarithm of the generalized jacobian determinant\n",
    "    log_dets = rm.metric_logdet(base_point=latent_activations)\n",
    "    # replace nan values with a small number\n",
    "    #EPSILON = 1e-9\n",
    "    #torch.nan_to_num(log_dets, nan=EPSILON, posinf=EPSILON, neginf=EPSILON)\n",
    "    torch.nan_to_num(log_dets, nan=1., posinf=1., neginf=1.) # ?? TODO Investigate replacement of eps by 1\n",
    "    # calculate the variance of the logarithm of the generalized jacobian determinant\n",
    "    raw_loss = torch.var(log_dets)\n",
    "    return raw_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "69af188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicatrix_volume_variance_metric(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e4c564c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from diffusion_curvature.kernels import gaussian_kernel\n",
    "import pygsp\n",
    "\n",
    "def frequency_of_volume_variance(\n",
    "    model,\n",
    "    dataloader,\n",
    "):\n",
    "    pointcloud = dataloader.dataset.pointcloud\n",
    "    try:\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "    except AttributeError:\n",
    "        latent_activations = model.encode(dataloader.dataset.pointcloud).cpu().detach()\n",
    "    \n",
    "    # construct a graph out of these latent_activations\n",
    "    W = gaussian_kernel(\n",
    "        X,\n",
    "        kernel_type='adaptive',\n",
    "        k = k,\n",
    "        anisotropic_density_normalization = alpha,\n",
    "    )\n",
    "    G = pygsp.graphs.Graph(W)\n",
    "    \n",
    "    # set up manifold\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "    # calculate the logarithm of the generalized jacobian determinant\n",
    "    log_dets = rm.metric_logdet(base_point=latent_activations)\n",
    "    # replace nan values with a small number\n",
    "    EPSILON = 1e-9\n",
    "    torch.nan_to_num(log_dets, nan=EPSILON, posinf=EPSILON, neginf=EPSILON)\n",
    "\n",
    "    # compute quadric form of laplacian\n",
    "    quadric_form = log_dets @ G.L @ log_dets.T\n",
    "    return quadric_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a0c36679",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_of_volume_variance(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4543bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from diffusion_curvature.kernels import gaussian_kernel\n",
    "import pygsp\n",
    "\n",
    "def frequency_of_volume_variance(\n",
    "    model,\n",
    "    dataloader,\n",
    "):\n",
    "    pointcloud = dataloader.dataset.pointcloud\n",
    "    try:\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach().numpy()\n",
    "    except AttributeError:\n",
    "        latent_activations = model.encode(dataloader.dataset.pointcloud).cpu().detach().numpy()\n",
    "    \n",
    "    # construct a graph out of these latent_activations\n",
    "    W = gaussian_kernel(\n",
    "        latent_activations,\n",
    "        kernel_type='adaptive',\n",
    "        k = k,\n",
    "        anisotropic_density_normalization = alpha,\n",
    "    )\n",
    "    G = pygsp.graphs.Graph(W)\n",
    "    \n",
    "    # set up manifold\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "    # calculate the logarithm of the generalized jacobian determinant\n",
    "    log_dets = rm.metric_logdet(base_point=latent_activations)\n",
    "    # replace nan values with a small number\n",
    "    EPSILON = 1e-9\n",
    "    torch.nan_to_num(log_dets, nan=EPSILON, posinf=EPSILON, neginf=EPSILON)\n",
    "\n",
    "    # compute quadric form of laplacian\n",
    "    quadric_form = log_dets @ G.L @ log_dets.T\n",
    "    return quadric_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f47e41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_of_volume_variance(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "551b4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from diffusion_curvature.kernels import gaussian_kernel\n",
    "import pygsp\n",
    "\n",
    "def frequency_of_volume_variance(\n",
    "    model,\n",
    "    dataloader,\n",
    "    k=5\n",
    "):\n",
    "    pointcloud = dataloader.dataset.pointcloud\n",
    "    try:\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach().numpy()\n",
    "    except AttributeError:\n",
    "        latent_activations = model.encode(dataloader.dataset.pointcloud).cpu().detach().numpy()\n",
    "    \n",
    "    # construct a graph out of these latent_activations\n",
    "    W = gaussian_kernel(\n",
    "        latent_activations,\n",
    "        kernel_type='adaptive',\n",
    "        k = k,\n",
    "        anisotropic_density_normalization = alpha,\n",
    "    )\n",
    "    G = pygsp.graphs.Graph(W)\n",
    "    \n",
    "    # set up manifold\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "    # calculate the logarithm of the generalized jacobian determinant\n",
    "    log_dets = rm.metric_logdet(base_point=latent_activations)\n",
    "    # replace nan values with a small number\n",
    "    EPSILON = 1e-9\n",
    "    torch.nan_to_num(log_dets, nan=EPSILON, posinf=EPSILON, neginf=EPSILON)\n",
    "\n",
    "    # compute quadric form of laplacian\n",
    "    quadric_form = log_dets @ G.L @ log_dets.T\n",
    "    return quadric_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3326d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_of_volume_variance(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e0f89966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from diffusion_curvature.kernels import gaussian_kernel\n",
    "import pygsp\n",
    "\n",
    "def frequency_of_volume_variance(\n",
    "    model,\n",
    "    dataloader,\n",
    "    k=5, # k-nn graph\n",
    "    alpha=1, # degree of anisotropic density normalization\n",
    "):\n",
    "    pointcloud = dataloader.dataset.pointcloud\n",
    "    try:\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach().numpy()\n",
    "    except AttributeError:\n",
    "        latent_activations = model.encode(dataloader.dataset.pointcloud).cpu().detach().numpy()\n",
    "    \n",
    "    # construct a graph out of these latent_activations\n",
    "    W = gaussian_kernel(\n",
    "        latent_activations,\n",
    "        kernel_type='adaptive',\n",
    "        k = k,\n",
    "        anisotropic_density_normalization = alpha,\n",
    "    )\n",
    "    G = pygsp.graphs.Graph(W)\n",
    "    \n",
    "    # set up manifold\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "    # calculate the logarithm of the generalized jacobian determinant\n",
    "    log_dets = rm.metric_logdet(base_point=latent_activations)\n",
    "    # replace nan values with a small number\n",
    "    EPSILON = 1e-9\n",
    "    torch.nan_to_num(log_dets, nan=EPSILON, posinf=EPSILON, neginf=EPSILON)\n",
    "\n",
    "    # compute quadric form of laplacian\n",
    "    quadric_form = log_dets @ G.L @ log_dets.T\n",
    "    return quadric_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8e550228",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_of_volume_variance(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0579425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from diffusion_curvature.kernels import gaussian_kernel\n",
    "import pygsp\n",
    "\n",
    "def frequency_of_volume_variance(\n",
    "    model,\n",
    "    dataloader,\n",
    "    k=5, # k-nn graph\n",
    "    alpha=1, # degree of anisotropic density normalization\n",
    "):\n",
    "    pointcloud = dataloader.dataset.pointcloud\n",
    "    try:\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach().numpy()\n",
    "    except AttributeError:\n",
    "        latent_activations = model.encode(dataloader.dataset.pointcloud).cpu().detach().numpy()\n",
    "    \n",
    "    # construct a graph out of these latent_activations\n",
    "    W = gaussian_kernel(\n",
    "        latent_activations,\n",
    "        kernel_type='adaptive',\n",
    "        k = k,\n",
    "        anisotropic_density_normalization = alpha,\n",
    "    )\n",
    "    G = pygsp.graphs.Graph(W)\n",
    "    \n",
    "    # set up manifold\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "    # calculate the logarithm of the generalized jacobian determinant\n",
    "    log_dets = rm.metric_logdet(base_point=latent_activations)\n",
    "    # replace nan values with a small number\n",
    "    EPSILON = 1e-9\n",
    "    torch.nan_to_num(log_dets, nan=EPSILON, posinf=EPSILON, neginf=EPSILON)\n",
    "    log_dets = log_dets.numpy()\n",
    "\n",
    "    # compute quadric form of laplacian\n",
    "    quadric_form = log_dets @ G.L @ log_dets.T\n",
    "    return quadric_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cfe54821",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_of_volume_variance(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "27b170c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from diffusion_curvature.kernels import gaussian_kernel\n",
    "import pygsp\n",
    "\n",
    "def frequency_of_volume_variance(\n",
    "    model,\n",
    "    dataloader,\n",
    "    k=5, # k-nn graph\n",
    "    alpha=1, # degree of anisotropic density normalization\n",
    "):\n",
    "    pointcloud = dataloader.dataset.pointcloud\n",
    "    try:\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "    except AttributeError:\n",
    "        latent_activations = model.encode(dataloader.dataset.pointcloud).cpu().detach()\n",
    "    \n",
    "    # construct a graph out of these latent_activations\n",
    "    W = gaussian_kernel(\n",
    "        latent_activations.numpy(),\n",
    "        kernel_type='adaptive',\n",
    "        k = k,\n",
    "        anisotropic_density_normalization = alpha,\n",
    "    )\n",
    "    G = pygsp.graphs.Graph(W)\n",
    "    \n",
    "    # set up manifold\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "    # calculate the logarithm of the generalized jacobian determinant\n",
    "    log_dets = rm.metric_logdet(base_point=latent_activations)\n",
    "    # replace nan values with a small number\n",
    "    EPSILON = 1e-9\n",
    "    torch.nan_to_num(log_dets, nan=EPSILON, posinf=EPSILON, neginf=EPSILON)\n",
    "    log_dets = log_dets.numpy()\n",
    "\n",
    "    # compute quadric form of laplacian\n",
    "    quadric_form = log_dets @ G.L @ log_dets.T\n",
    "    return quadric_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0f7c2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_of_volume_variance(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc353975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from diffusion_curvature.kernels import gaussian_kernel\n",
    "import pygsp\n",
    "\n",
    "def frequency_of_volume_variance(\n",
    "    model,\n",
    "    dataloader,\n",
    "    k=5, # k-nn graph\n",
    "    alpha=1, # degree of anisotropic density normalization\n",
    "):\n",
    "    pointcloud = dataloader.dataset.pointcloud\n",
    "    try:\n",
    "        latent_activations = model.encoder(dataloader.dataset.pointcloud).cpu().detach()\n",
    "    except AttributeError:\n",
    "        latent_activations = model.encode(dataloader.dataset.pointcloud).cpu().detach()\n",
    "    \n",
    "    # construct a graph out of these latent_activations\n",
    "    W = gaussian_kernel(\n",
    "        latent_activations.numpy(),\n",
    "        kernel_type='adaptive',\n",
    "        k = k,\n",
    "        anisotropic_density_normalization = alpha,\n",
    "    )\n",
    "    G = pygsp.graphs.Graph(W)\n",
    "    \n",
    "    # set up manifold\n",
    "    pbm = PullbackMetric(2, model.decoder)\n",
    "    lcc = LeviCivitaConnection(2, pbm)\n",
    "    rm = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "    # calculate the logarithm of the generalized jacobian determinant\n",
    "    log_dets = rm.metric_logdet(base_point=latent_activations)\n",
    "    # replace nan values with a small number\n",
    "    EPSILON = 1e-9\n",
    "    torch.nan_to_num(log_dets, nan=EPSILON, posinf=EPSILON, neginf=EPSILON)\n",
    "    log_dets = log_dets.detach().cpu().numpy()\n",
    "\n",
    "    # compute quadric form of laplacian\n",
    "    quadric_form = log_dets @ G.L @ log_dets.T\n",
    "    return quadric_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5403c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_of_volume_variance(model, trainloader)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
