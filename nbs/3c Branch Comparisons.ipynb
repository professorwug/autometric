{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3c Branch Comparisons\n",
    "> What do pullback metrics tell us about how well we embed the branch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question: *what can the encoder pullback metric tell us about the quality of the encoding?* \n",
    "\n",
    "The branch dataset gives us a sequence of 1-manifolds which *can* be embedded into 2d, along with attributes on how difficult it should be to embed them in 2D: how many branching points are there, and how many branches come out of these points? What are the lengths of each branch?\n",
    "\n",
    "The encoder pullback metric can give us several types of information, with which we can try to recover the above features.\n",
    "\n",
    "1. The local dimension of the pullback defined at each point, which corresponds to its dimension in the embedding space. The maximum dimension here is $d$, the embedding dimension -- but if it's a 1 manifold being embedded, perhaps along straight branches, this will be $1$. Likewise, at branching points, it should be higher.\n",
    "2. In addition to the dimension, we can investigate the *distribution* of the eigenvalues of the metric, perhaps considering their spectral entropy. I expect the relative size of the second eval to the first could indicate the severity of the branching point.\n",
    "3. The eigenvectors of the encoder pullback metric tell us which directions are most contracted (the first eigenvector is the most contracted dimension). We could comapre this to the lengths of each of the branches."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
