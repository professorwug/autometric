{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp self_evaluating_datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"GEOMSTATS_BACKEND\"] = \"pytorch\"\n",
    "\n",
    "# models\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autometric's Self Evaluating Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastcore.all import *\n",
    "import inspect\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "def metric(func):\n",
    "    setattr(func, 'tag', 'metric')\n",
    "    return func\n",
    "\n",
    "class Wrapper:\n",
    "    def __init__(self, obj, **kwargs):\n",
    "        self.obj = obj\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "class SelfEvaluatingDataset():\n",
    "    def __init__(self,\n",
    "                 datalist:List, # list of objects to be evaluated in the dataset. Usually includes multiple examples, e.g. a torus, sphere, saddle; multiple images, multiple validation datasets.\n",
    "                 names:List, # names of the datasets in datalist.\n",
    "                 result_names:List, # quantities to be computed (e.g. curvature, predictions). Usually just one per dataset.\n",
    "                 save_directory:str = \".self-evaluating-datasets\", \n",
    "                ):\n",
    "        store_attr()\n",
    "\n",
    "        self.DS = [ # list of datasets\n",
    "            Wrapper(obj, results={rn:{} for rn in result_names}, name=name) for obj, name in zip(datalist, names)\n",
    "        ]\n",
    "        print(\"I am current\")\n",
    "        self.idx = -1\n",
    "        for i in range(self.__len__()):\n",
    "            # aggregate ground truth values\n",
    "            for rn in self.result_names:\n",
    "                self._store_truth(rn, i)\n",
    "\n",
    "        # create the save directory if needed\n",
    "        # Check if the directory exists\n",
    "        if not os.path.exists(save_directory):\n",
    "            # Create the directory\n",
    "            os.makedirs(save_directory)\n",
    "        else:\n",
    "            # try loading previous runs\n",
    "            self.load_from_disk()\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.DS)\n",
    "    \n",
    "    def preprocess(self, unprocessed_data_object):\n",
    "        return unprocessed_data_object # override\n",
    "    \n",
    "    def get_item(self, idx):\n",
    "        return self.DS[idx]\n",
    "\n",
    "    def save_results(self):\n",
    "        # saves DS to the save_directory using joblib dump\n",
    "        child_name = type(self).__name__\n",
    "        filename = os.path.join(self.save_directory, child_name + '.joblib_dump')\n",
    "        joblib.dump(self.DS, filename)\n",
    "\n",
    "    def load_from_disk(self):\n",
    "        # loads saved DS from dict using joblib\n",
    "        child_name = type(self).__name__\n",
    "        filename = os.path.join(self.save_directory, child_name + '.joblib_dump')\n",
    "        if os.path.exists(filename):\n",
    "            self.DS = joblib.load(filename)\n",
    "            r = self.DS[0].results\n",
    "            imported_methods = list(r[list(r.keys())[0]].keys())\n",
    "            print(f\"Loaded evaluations on methods {imported_methods}\")\n",
    "        else:\n",
    "            print(\"No previously saved evaluations. Starting from 0.\")\n",
    "\n",
    "    def __next__(self):\n",
    "        self.idx += 1\n",
    "        if self.idx >= self.__len__():\n",
    "            self.save_results()\n",
    "            raise StopIteration\n",
    "        result = self.get_item(self.idx)\n",
    "        return result\n",
    "\n",
    "    def update(self,\n",
    "               result,\n",
    "               idx = None,\n",
    "               result_name = 'default',\n",
    "               method_name='computed',\n",
    "               ):\n",
    "        \"\"\"\n",
    "        Store the result of the curvature computation by passing the computed curvature of the center (first) point.\n",
    "        \"\"\"\n",
    "        if idx is None: idx = self.idx\n",
    "        if result_name == 'default': \n",
    "            if len(self.result_names) == 1:\n",
    "                result_name = self.result_names[0]\n",
    "        self.DS[idx].results[result_name][method_name] = result\n",
    "\n",
    "    def get_truth(self, result_name, idx):\n",
    "        \"\"\"Compute the ground truth for each of your targets, and assign to a method. Usually this involves accessing some attribute of the input data and calling the update function\"\"\"\n",
    "        truth = None\n",
    "        return truth\n",
    "\n",
    "    \n",
    "    def _store_truth(self, result_name, idx):\n",
    "        truth = self.get_truth(result_name, idx)\n",
    "        self.update(\n",
    "            truth, idx, method_name = \"ground truth\", result_name=result_name\n",
    "        )\n",
    "\n",
    "\n",
    "    def compute_metrics(self, labels, filter = None):\n",
    "        metrics = self._get_metrics()\n",
    "        metric_tables = {rn : {} for rn in self.result_names}\n",
    "        for rn in self.result_names:\n",
    "            for metric in metrics:\n",
    "                metric_tables[rn][metric.__name__] = {}\n",
    "                for method_name in self.method_names:\n",
    "                    metric_tables[rn][metric.__name__][method_name] = self.compute(metric=metric, method_name=method_name, result_name=rn, labels = labels, filter = None)\n",
    "            metric_tables[rn] = pd.DataFrame(metric_tables[rn])\n",
    "        return metric_tables\n",
    "            \n",
    "    def compute(self, metric, result_name, method_name, labels, filter=None):\n",
    "        # Overwrite this class with your logic. It implements the computation of a single metric for a single method\n",
    "        return metric(labels[result_name][method_name], labels[result_name]['ground truth'], result_name = result_name)\n",
    "    \n",
    "\n",
    "    def _aggregate_labels(self):\n",
    "        # returns a dictionary whose keys are method names, paired with a list of each of the results given by the metrics.\n",
    "        # Just a more convenient data format for comparing method outputs.\n",
    "        self.method_names = list(self.DS[0].results[self.result_names[0]].keys())\n",
    "        self.labels = {}\n",
    "        for rn in self.result_names:\n",
    "            self.labels[rn] = {}\n",
    "            for m in self.method_names:\n",
    "                self.labels[rn][m] = [self.DS[i].results[rn][m] for i in range(self.__len__())]\n",
    "\n",
    "\n",
    "    def plot(self, title = None):\n",
    "        if title is None: title = f\"In dimension {self.dimension}\"\n",
    "        # for each computed method on this dataset, we plot the histogram of saddles vs spheres\n",
    "        self._aggregate_labels()\n",
    "        # get the idxs for each type of dataset\n",
    "        dataset_names = [self.DS.data_vars[i].attrs['name'] for i in range(len(self.DS))]\n",
    "        unique_names = list(set(dataset_names))\n",
    "        idxs_by_name = {n: [i for i, name in enumerate(dataset_names) if name == n] for n in unique_names}        \n",
    "        for m in self.method_names: \n",
    "            if m != 'ks' and m != 'name':\n",
    "                for dname in unique_names:\n",
    "                    plt.hist(self.labels[m][idxs_by_name[dname]], bins=50, label = dname, edgecolor='none', linewidth=5)\n",
    "                plt.legend()\n",
    "                plt.xlabel(m)\n",
    "                plt.title(title)\n",
    "                plt.show()\n",
    "\n",
    "    def table(self, filter=None):\n",
    "        self._aggregate_labels()\n",
    "        self.metric_tables = self.compute_metrics(filter=filter, labels = self.labels)\n",
    "        for k in self.metric_tables.keys():\n",
    "            print(k)\n",
    "            print(self.metric_tables[k])\n",
    "        return self.metric_tables\n",
    "\n",
    "    def _get_metrics(self):\n",
    "        tagged_functions = []\n",
    "        methods = [method for method in dir(self) if callable(getattr(self, method))]\n",
    "        for method_name in methods:\n",
    "            member = getattr(self, method_name)\n",
    "            if hasattr(member, 'tag') and getattr(member, 'tag') == 'metric':\n",
    "                tagged_functions.append(member)\n",
    "        return tagged_functions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "Red\n"
     ]
    }
   ],
   "source": [
    "dls = [3,1,4,1,5,9,2,6,5]\n",
    "class Wrapper:\n",
    "    def __init__(self, obj, **kwargs):\n",
    "        self.obj = obj\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "# Example usage with a list of strings (but can be any objects)\n",
    "original_objects = [\"Apple\", \"Banana\"]\n",
    "\n",
    "wrapped_objects = [\n",
    "    Wrapper(obj, color=\"Red\", weight=150) for obj in original_objects\n",
    "]\n",
    "\n",
    "# Accessing attributes\n",
    "print(wrapped_objects[0].obj)  # Output: Apple\n",
    "print(wrapped_objects[0].color)  # Output: Red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Red'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_objects[1].__dict__['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ \u001b[1mPixi task (\u001b[0m\u001b[35m\u001b[1mdefault\u001b[0m\u001b[1m): \u001b[0m\u001b[34m\u001b[1mnbdev_export\u001b[0m\n",
      "\u001b[2K\u001b[32m⠁\u001b[0m activating environment                                                                 "
     ]
    }
   ],
   "source": [
    "!pixi run nbsync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmae-p",
   "language": "python",
   "name": "dmae-p"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
