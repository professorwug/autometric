{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp geodesics\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geodesics\n",
    "> A JAX Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a literate-programming adaptation of Edward de Brouwer's `fm_geodesics` library, programmed in jax for speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax Training Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice feature of jax flax: it infers the input dimensions on initialization. They needn't be hard coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn \n",
    "\n",
    "class MLP(nn.Module):\n",
    "  \"\"\"\n",
    "  A general MLP in jax\n",
    "  \"\"\"\n",
    "  num_hidden_layers: int\n",
    "  hidden_dim: int\n",
    "  output_dim: int\n",
    "  \n",
    "  def setup(self):\n",
    "    if self.num_hidden_layers == 0:\n",
    "       self.layers = [nn.Dense(self.output_dim)]\n",
    "    else:\n",
    "      input_layer = [nn.Dense(self.hidden_dim)]\n",
    "      hidden_layers = [nn.Dense(self.hidden_dim) for _ in range(self.num_hidden_layers -1)]\n",
    "      output_layer = [nn.Dense(self.output_dim)]\n",
    "      self.layers = input_layer + hidden_layers + output_layer\n",
    "\n",
    "  def __call__(self,x):\n",
    "      for i, layer in enumerate(self.layers):\n",
    "        x = layer(x)\n",
    "        if i < len(self.layers) - 1: # no activation for the last layer\n",
    "          x = nn.relu(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import wandb\n",
    "\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "def compute_and_log_metrics(state, metrics_history, step_num, prefix = \"train_\", logger = True, commit = True):\n",
    "    \"\"\"\n",
    "    Compute metrics and log them with wandb\n",
    "\n",
    "    commit: wether this will be the last logged value for this step or not.\n",
    "    \"\"\"\n",
    "    metric_dict = state.metrics.compute().items()\n",
    "    for i_, (metric,value) in enumerate(metric_dict): # compute metrics\n",
    "        metrics_history[prefix + metric].append(value) # record metrics\n",
    "\n",
    "        # Log Metrics to Weights & Biases\n",
    "        if logger:\n",
    "            if commit:\n",
    "                if i_ == len(metric_dict)-1:  \n",
    "                    wandb.log({prefix + metric:value.item()}, step = step_num, commit = True)\n",
    "                    continue\n",
    "            wandb.log({prefix + metric:value.item()}, step = step_num, commit = False)\n",
    "    \n",
    "    return metrics_history\n",
    "\n",
    "\n",
    "def get_pylogger(name=__name__) -> logging.Logger:\n",
    "    \"\"\"Initializes multi-GPU-friendly python command line logger.\"\"\"\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "\n",
    "    # this ensures all logging levels get marked with the rank zero decorator\n",
    "    # otherwise logs would get multiplied for each GPU process in multi-GPU setup\n",
    "    logging_levels = (\"debug\", \"info\", \"warning\", \"error\", \"exception\", \"fatal\", \"critical\")\n",
    "    for level in logging_levels:\n",
    "        setattr(logger, level, rank_zero_only(getattr(logger, level)))\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pytorch Lightning like trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tqdm\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import wandb\n",
    "\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "class JaxTrainer:\n",
    "    \"\"\"\n",
    "    Functions to implement:\n",
    "    - train_step\n",
    "\n",
    "    Attributes to give:\n",
    "    - metrics_names : the list of names of the different metrics that are recorded.\n",
    "    - max_epochs : the number of epochs\n",
    "    \"\"\"\n",
    "    def __init__(self, datamodule):\n",
    "        self.train_dl = datamodule.train_dataloader()\n",
    "        self.val_dl = datamodule.val_dataloader()\n",
    "        self.test_dl = datamodule.test_dataloader()\n",
    "\n",
    "    def fit(self):\n",
    "        step = 0\n",
    "        self.metrics_history = { **{\"train_\" + m : [] for m in self.metrics_names}, **{\"val_\" + m : [] for m in self.metrics_names}, **{\"test_\" + m : [] for m in self.metrics_names}} \n",
    "        \n",
    "        for epoch in range(self.max_epochs):\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            self.epoch = epoch\n",
    "\n",
    "            for train_step,batch in tqdm.tqdm(enumerate(self.train_dl)):\n",
    "                self.train_step(step, batch)\n",
    "                self.train_step_end(step)\n",
    "                step+=1\n",
    "            self.train_epoch_end(step)\n",
    "\n",
    "            self.val_state = self.state\n",
    "            for val_step, val_batch in tqdm.tqdm(enumerate(self.val_dl)):\n",
    "                self.val_step(step, val_batch)\n",
    "                self.val_step_end(step)\n",
    "            self.val_epoch_end(step)\n",
    "            \n",
    "            if self.logger:\n",
    "                wandb.log({\"epoch\": epoch}, step = step)\n",
    "            step = step + 1\n",
    "\n",
    "            for callbacks in self.callbacks:\n",
    "                callbacks.on_epoch_end(self)\n",
    "            \n",
    "        self.test_state = self.state\n",
    "        for test_step, test_batch in tqdm.tqdm(enumerate(self.test_dl)):\n",
    "            self.test_step(step, test_batch)\n",
    "            self.test_step_end(step)\n",
    "        self.test_epoch_end(step)\n",
    "\n",
    "        for callbacks in self.callbacks:\n",
    "            callbacks.on_training_end(self)\n",
    "        \n",
    "        if self.logger:\n",
    "            wandb.log({\"epoch\": epoch + 1}, step = step)\n",
    "    \n",
    "    def train_step_end(self,step):\n",
    "\n",
    "        if ((step % self.log_every_n_steps) == 0):\n",
    "            self.metrics_history = compute_and_log_metrics(self.state, metrics_history= self.metrics_history, step_num = step, prefix = \"train_\", logger = self.logger, commit = True)\n",
    "\n",
    "    def train_epoch_end(self,step):\n",
    "        self.metrics_history = compute_and_log_metrics(self.state, metrics_history= self.metrics_history, step_num = step, prefix = \"train_\", logger = self.logger, commit = False)\n",
    "        self.state = self.state.replace(metrics=self.state.metrics.empty()) # reset train_metrics for next training epoch\n",
    "\n",
    "    def val_step_end(self,step):\n",
    "        return\n",
    "    \n",
    "    def val_epoch_end(self,step):\n",
    "        metrics_history = compute_and_log_metrics(self.val_state, metrics_history= self.metrics_history, step_num = step , prefix = \"val_\", logger = self.logger, commit = False)\n",
    "        self.val_state = self.val_state.replace(metrics=self.val_state.metrics.empty()) # reset metrics\n",
    "\n",
    "    def test_step_end(self,step):\n",
    "        return\n",
    "    \n",
    "    def test_epoch_end(self,step):\n",
    "        self.metrics_history = compute_and_log_metrics(self.test_state, metrics_history= self.metrics_history, step_num = step , prefix = \"test_\", logger = self.logger, commit = False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trainable curve, which takes a linear interpolation between the input points $x_0$ and $x_1$, and uses the MLP `mod_x0_x1` to send a vector containing a stacked representation of `[x0, x1, t]` to a point in latent space, for each time point. These points computed from teh stacked representation are then scaled by a window function $s(q - (2t - 1)^{2e})$ and added to the linear interpolation. The window guarantees that the endpoints aren't perturbed, by scaling the addition in this part of the time window to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "from clu import metrics \n",
    "from flax import struct  \n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from flax import linen as nn \n",
    "import numpy as np\n",
    "\n",
    "  \n",
    "\n",
    "class CondCurve(nn.Module):\n",
    "  \"\"\"\n",
    "  Conditional Grounded Curve\n",
    "  \"\"\"\n",
    "  input_dim: int\n",
    "  hidden_dim: int\n",
    "  scale_factor: int\n",
    "  symmetric: bool\n",
    "  num_layers: int\n",
    "  envelope_exponent: int = 1\n",
    "  exponent_as_parameter: bool = False\n",
    "\n",
    "  def setup(self):\n",
    "    self.mod_x0_x1 = MLP( hidden_dim = self.hidden_dim, output_dim = self.input_dim, num_hidden_layers = self.num_layers)\n",
    "      \n",
    "    self.x0_emb = MLP( hidden_dim=self.hidden_dim, output_dim = self.hidden_dim, num_hidden_layers = self.num_layers)\n",
    "    self.x1_emb = MLP( hidden_dim=self.hidden_dim, output_dim = self.hidden_dim, num_hidden_layers = self.num_layers)\n",
    "\n",
    "    if self.exponent_as_parameter:\n",
    "      exponent_init = nn.initializers.constant(self.envelope_exponent)\n",
    "      self.var_envelope_exponent = self.param('var_envelope_exponent',\n",
    "                          exponent_init, # Initialization function\n",
    "                          (1,))  # shape info.\n",
    "  \n",
    "  def x0_emb_(self,x):\n",
    "    ### Just for debugging\n",
    "    return self.x0_emb(x)\n",
    "  \n",
    "  def x1_emb_(self,x):\n",
    "    ### Just for debugging\n",
    "    return self.x1_emb(x)\n",
    "  \n",
    "  def mod_x0_x1_(self,x):\n",
    "    ### Just for debugging\n",
    "    return self.mod_x0_x1(x)\n",
    "  \n",
    "  def __call__(self,x0, x1, t):\n",
    "\n",
    "    x0_ = jnp.tile(x0, (t.shape[0],1))\n",
    "    x1_ = jnp.tile(x1, (t.shape[0],1))\n",
    "    t_ = jnp.repeat(t, x0.shape[0])[:,None]\n",
    "  \n",
    "    emb_x0 = self.x0_emb(x0_)\n",
    "    emb_x1 = self.x1_emb(x1_)\n",
    "\n",
    "    avg = t_ * x1_ + (1-t_) * x0_\n",
    "\n",
    "    if self.exponent_as_parameter:\n",
    "      env_exp = self.var_envelope_exponent\n",
    "    else:\n",
    "      env_exp = self.envelope_exponent\n",
    "    \n",
    "    envelope = self.scale_factor * (1- (t_*2-1)** (2 * env_exp)) \n",
    "    aug_state = jnp.concatenate([emb_x0, emb_x1,t_], axis = -1)\n",
    "    outs =  self.mod_x0_x1(aug_state) * envelope + avg \n",
    "\n",
    "    return outs.reshape(t.shape[0],x0.shape[0], self.input_dim)\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "  #accuracy: metrics.Accuracy\n",
    "  loss: metrics.Average.from_output('loss') # overall loss\n",
    "  true_length: metrics.Average.from_output('true_length') # keeps track of the true length of each geodesic\n",
    "  mse_geodesic: metrics.Average.from_output('mse_geodesic') # compute MSE between true and predicted geodesics\n",
    "  loss_geo: metrics.Average.from_output('loss_geo') # loss for the geodesic network\n",
    "  loss_density: metrics.Average.from_output('loss_density') # loss for the density network\n",
    "  \n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "  metrics: Metrics\n",
    "\n",
    "def create_geodesic_train_state(module, rng, learning_rate):\n",
    "  \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "  params = module.init(rng, jnp.ones([1, module.input_dim]), jnp.ones([1, module.input_dim]), jnp.ones([3]))['params'] # initialize parameters by passing a template image\n",
    "  tx = optax.adam(learning_rate)\n",
    "  return TrainState.create(\n",
    "      apply_fn=module.apply, params=params, tx=tx,\n",
    "      metrics=Metrics.empty())\n",
    "\n",
    "def jacdiag(f ):\n",
    "    def _jacdiag(theta, x0,x1, t):\n",
    "        def partial_grad_f_index(i):\n",
    "            return jax.jacrev(f,argnums = 3)(theta, x0,x1,t[i][None])[0,...,0]\n",
    "        return jax.vmap(partial_grad_f_index)(jax.numpy.arange(t.shape[0]))\n",
    "    return _jacdiag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geodesic Trainer, which takes a datamodule, parameterizable curve, and metric function.\n",
    "\n",
    "It also takes an 'oracle', which returns the true lengths of geodesics between points in the given space - but this is used solely for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class GeodesicTrainer(JaxTrainer):\n",
    "\n",
    "    def __init__(self, \n",
    "                 datamodule, # a class with functions train_dataloader, val_dataloader, test_dataloader which return those. Each batch should return two points, x_o, and x_1\n",
    "                 cond_curve, # the curve to be learned\n",
    "                 metric, # a function that, given (n, d) input, returns the metric for each of n points.\n",
    "                 oracle, # \n",
    "                 max_epochs, \n",
    "                 seed, \n",
    "                 lr,\n",
    "                 log_every_n_steps,\n",
    "                 n_interp_times, \n",
    "                 density_lambda, k_density, logger, callbacks,  **kwargs):\n",
    "\n",
    "        super().__init__(datamodule)\n",
    "        self.model = cond_curve\n",
    "\n",
    "        self.geo_metric = metric\n",
    "        self.geo_oracle = oracle\n",
    "        \n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "        self.density_lambda = density_lambda\n",
    "        self.k_density = k_density\n",
    "\n",
    "        if self.density_lambda != 0:\n",
    "            x_train = datamodule.ds.tensors[0][datamodule.train_idx].double().numpy().copy() \n",
    "            self.X_train = jnp.array(x_train)\n",
    "        else:\n",
    "            self.X_train = None\n",
    "\n",
    "        self.t = jnp.linspace(0,1, n_interp_times)\n",
    "\n",
    "        init_rng = jax.random.key(seed)\n",
    "\n",
    "        self.state = create_geodesic_train_state(self.model, init_rng, lr)\n",
    "\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "\n",
    "        self.N_train = len(self.train_dl)\n",
    "\n",
    "        self.logger = logger # wether to log (wandb ) or not.\n",
    "\n",
    "        self.callbacks = callbacks\n",
    "        self.metrics_names = [m for m in list(self.state.metrics.__dict__.keys()) if '_reduction_counter' not in m]\n",
    "\n",
    "    def val_step(self,step, batch):\n",
    "        x0 = batch[0] #x0_fixed\n",
    "        x1 = batch[1] #x1_fixed\n",
    "        val_batch = x0, x1, self.t\n",
    "\n",
    "        self.val_state = self.compute_metrics(state=self.val_state, metric = self.geo_metric, oracle = self.geo_oracle, batch= val_batch, val = True)\n",
    "\n",
    "    def train_step(self, step, batch):\n",
    "\n",
    "        x0 = batch[0] #x0_fixed\n",
    "        x1 = batch[1] #x1_fixed\n",
    "        batch = x0, x1, self.t\n",
    "        \n",
    "        self.state = self.train_step_(self.state, self.geo_metric, batch) # get updated train state (which contains the updated parameters)\n",
    "        self.state = self.compute_metrics(state=self.state, metric = self.geo_metric, oracle = self.geo_oracle, batch=batch) # aggregate batch metrics\n",
    "    \n",
    "    def test_step(self, step, batch):\n",
    "    \n",
    "        x0 = batch[0] #x0_fixed\n",
    "        x1 = batch[1] #x1_fixed\n",
    "        test_batch = x0, x1, self.t\n",
    "        self.test_state = self.compute_metrics(state=self.test_state, metric = self.geo_metric, oracle = self.geo_oracle, batch= test_batch, val = True)\n",
    "\n",
    "    def density_loss_fn(self, out):\n",
    "        \n",
    "        key = jax.random.key(421)\n",
    "        out_ = out[1:-1].reshape(-1, out.shape[-1]) # remove the first and last points which are fixed.\n",
    "        #breakpoint()\n",
    "        #X_train = jax.random.normal(key,shape= (3000,out_.shape[-1]))\n",
    "        #X_train = np.random.randn(3000, out_.shape[-1])\n",
    "        \n",
    "        def euclidean_norm(a,b):\n",
    "            return jnp.sqrt(jnp.sum((a-b)**2, axis = -1))\n",
    "\n",
    "        mv = vmap(euclidean_norm, in_axes = (0,None))   \n",
    "        mm = vmap(mv, in_axes = (None,0), out_axes = 1)     \n",
    "        #cdist = vmap(vmap(euclidean_norm, in_axes=(None,0)), in_axes=(0,None))\n",
    "        dists = mm(out_, self.X_train)\n",
    "\n",
    "        val, idx = jax.lax.top_k(-dists, self.k_density) \n",
    "        return -jnp.mean(val)\n",
    "        dists = -jnp.transpose(dists,(1,2,0))\n",
    "        val, idx = jax.lax.top_k(dists, self.k_density)\n",
    "\n",
    "        return -jnp.mean(val)\n",
    "\n",
    "    def loss_fn(self,state, params, metric, x0, x1, t):\n",
    "        \"\"\"\n",
    "        Loss function for the geodesic network\n",
    "        \"\"\"\n",
    "\n",
    "        jac = jacdiag(state.apply_fn)({'params':params},x0,x1,t)\n",
    "\n",
    "        out = state.apply_fn({'params':params}, x0, x1, t)\n",
    "        mu = metric(out)\n",
    "\n",
    "        pre_prod = jnp.einsum('tbd,tbdj->tbj',jac,mu)\n",
    "        prod = jnp.einsum('tbd,tbd->tb', pre_prod, jac)\n",
    "\n",
    "        loss_geo = jnp.sqrt(prod).mean(0).sum()\n",
    "        \n",
    "        loss_geo = jnp.nan_to_num(loss_geo)\n",
    "\n",
    "        if self.density_lambda != 0:\n",
    "            loss_density =  self.density_loss_fn(out)\n",
    "        else:\n",
    "            loss_density = 0\n",
    "\n",
    "        loss = loss_geo + self.density_lambda * loss_density\n",
    "\n",
    "        return loss, (loss_geo, loss_density), out\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,2))\n",
    "    def train_step_(self,state, metric, batch):\n",
    "        \"\"\"Train for a single step.\"\"\"\n",
    "        x0, x1, t = batch\n",
    "        \n",
    "        def loss_fn_(params):\n",
    "\n",
    "            loss, _, out = self.loss_fn(state, params, metric, x0, x1, t)\n",
    "            #if self.density_lambda != 0:\n",
    "            #    loss_density =  self.density_loss_fn(out, X_train)\n",
    "            #else:\n",
    "            #    loss_density = 0\n",
    "            \n",
    "            #loss = loss_geo + loss_density\n",
    "            return loss\n",
    "\n",
    "        grad_fn = jax.grad(loss_fn_)\n",
    "        grads = grad_fn(state.params)\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,2,3,5,))\n",
    "    def compute_metrics(self,state, metric, oracle, batch, val = False):\n",
    "        \"\"\"\n",
    "        val : if true, computes some extra metrics which are only computed in val step.\n",
    "        \"\"\"\n",
    "\n",
    "        x0, x1, t = batch\n",
    "\n",
    "        loss, (loss_geo, loss_density), preds = self.loss_fn(state, state.params, metric, x0, x1, t)\n",
    "\n",
    "        geo_length = oracle.geo_length(x0,x1).sum() #true length\n",
    "\n",
    "        if val:\n",
    "            mse_geodesic_ = oracle.mse_geodesic(x0,x1,t, preds = preds)\n",
    "        else:\n",
    "            mse_geodesic_ = 0\n",
    "\n",
    "        metric_updates = state.metrics.single_from_model_output(loss=loss, true_length = geo_length, mse_geodesic = mse_geodesic_, loss_geo = loss_geo, loss_density = loss_density)\n",
    "        metrics = state.metrics.merge(metric_updates)\n",
    "        state = state.replace(metrics=metrics)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class pairwise_geodesics_dataset(Dataset):\n",
    "    def __init__(self, X, Y) -> None:\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target = self.Y[index, :]\n",
    "        sample = self.X[index, :]\n",
    "        return sample, target\n",
    "\n",
    "class GeodesicDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, X, Y, batch_size=32, split = [0.8,0.1,0.1]):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.n_obs = X.shape[0]\n",
    "        self.dim = X.shape[1]\n",
    "        self.batch_size = batch_size\n",
    "        self.split = split\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        last_train_idx = int(len(self.X) * self.split[0])\n",
    "        last_val_idx = int(len(self.X) * (self.split[0] + self.split[1]))\n",
    "        self.train_data = pairwise_geodesics_dataset(self.X[:last_train_idx], self.Y[:last_train_idx])\n",
    "        self.val_data = pairwise_geodesics_dataset(self.X[last_train_idx:last_val_idx], self.Y[last_train_idx:last_val_idx])\n",
    "        self.test_data = pairwise_geodesics_dataset(self.X[-last_val_idx:], self.Y[-last_val_idx:])\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        return\n",
    "        #self.train_loader = train_dataloader(self.name, self.n_obs, self.dim, self.emb_dim, self.batch_size, self.knn, self.PATH, self.indx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset = self.train_data, batch_size = self.batch_size, shuffle = True,\n",
    "                                           num_workers = 0, pin_memory = True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset = self.val_data, batch_size = self.batch_size, shuffle = True,\n",
    "                                           num_workers = 0, pin_memory = True)\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(dataset = self.test_data, batch_size = self.batch_size, shuffle = True,\n",
    "                                           num_workers = 0, pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by testing this on our old friend, the Sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphereMetric:\n",
    "    \"\"\"\n",
    "    Jax implementation of the metric on the sphere\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        m = jnp.tile(jnp.eye(2)[None,...],x.shape[:-1]+(1,1))\n",
    "        m = m.at[...,0,0].set(jnp.sin( x[...,-1])**(2))\n",
    "        return m\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return 0\n",
    "    \n",
    "\n",
    "class OracleSphere:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def geo_length(self,x0,x1):\n",
    "        \"\"\"\n",
    "        Computes the length between x0 and x1 on the sphere\n",
    "        \"\"\"\n",
    "        delta_l = jnp.abs(x0[:,0]-x1[:,0])\n",
    "        d = jnp.arccos(jnp.sin(x0[:,1]) * jnp.sin(x1[:,1]) + jnp.cos(x0[:,1])*jnp.cos(x1[:,1]) * jnp.cos(delta_l))\n",
    "        return d\n",
    "    \n",
    "    def mse_geodesic(self, x0,x1,t, preds):\n",
    "        return mse_geodesic(x0,x1,t, ground_truth = geodesic_sphere_jax, pred = preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Unable to use CUDA because of the following issues with CUDA components:\n",
      "Outdated cuSPARSE installation found.\n",
      "Version JAX was built against: 12200\n",
      "Minimum supported: 12100\n",
      "Installed version: 12002\n",
      "The local installation version must be no lower than 12100. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[[0., 0.],\n",
       "        [0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SM = SphereMetric()\n",
    "SM(jnp.zeros((1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the sphere metric here is in latent space, not ambient space. Does this mean that the NeuralFIM is likewise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autometric.datasets import Sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphere1 = Sphere(2000).X\n",
    "sphere2 = Sphere(2000).X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = GeodesicDataModule(sphere1, sphere2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_module = CondCurve(\n",
    "    input_dim = 2,\n",
    "    hidden_dim = 16,\n",
    "    scale_factor = 5,\n",
    "    symmetric = False,\n",
    "    num_layers=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GeodesicTrainer(\n",
    "    datamodule,\n",
    "    cond_curve = line_module,\n",
    "    metric = SM,\n",
    "    oracle = OracleSphere(),\n",
    "    max_epochs = 100,\n",
    "    lr = 0.001,\n",
    "    density_lambda=0,\n",
    "    seed = 421,\n",
    "    log_every_n_steps=10,\n",
    "    n_interp_times=20,\n",
    "    k_density=5, # default parameters in EB's hydra configs\n",
    "    logger = False,\n",
    "    callbacks = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sync changes to the library\n",
    "from IPython.display import display, Javascript\n",
    "import time\n",
    "display(Javascript('IPython.notebook.save_checkpoint();'))\n",
    "time.sleep(2)\n",
    "!pixi run nbsync"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
