{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#|default_exp self-evaluating-datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import deepdish\n",
    "\n",
    "import os\n",
    "os.environ[\"GEOMSTATS_BACKEND\"] = \"pytorch\"\n",
    "\n",
    "# models\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastcore.all import *\n",
    "import inspect\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def metric(func):\n",
    "    setattr(func, 'tag', 'metric')\n",
    "    return func\n",
    "\n",
    "class Wrapper:\n",
    "    def __init__(self, obj, **kwargs):\n",
    "        self.obj = obj\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "class SelfEvaluatingDataset():\n",
    "    def __init__(self,\n",
    "                 datalist:List, # list of objects to be evaluated in the dataset. Usually includes multiple examples, e.g. a torus, sphere, saddle; multiple images, multiple validation datasets.\n",
    "                 names:List, # names of the datasets in datalist.\n",
    "                 dimension = 2, # Dimension of saddles and spheres\n",
    "                 num_pointclouds = 100, # num pointclouds to make in total\n",
    "                 num_points = 2000, # num points per pointclouds\n",
    "                 noise_level = 0, # from 0 to 1. 1 is all noise.\n",
    "                 include_planes = False, # if True, includes randomly sampled planes as a sanity check.\n",
    "                ):\n",
    "        store_attr()\n",
    "        self.DS = [ # list of datasets\n",
    "            Wrapper(obj, results={}, name=name) for obj, name in zip(datalist, names)\n",
    "        ]\n",
    "        self.idx = -1\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.DS)\n",
    "    \n",
    "    def preprocess(self, unprocessed_data_object):\n",
    "        return unprocessed_data_object # override\n",
    "\n",
    "    def __next__(self):\n",
    "        self.idx += 1\n",
    "        if self.idx >= self.__len__():\n",
    "            raise StopIteration\n",
    "        result = self.DS[self.idx]\n",
    "        return result\n",
    "\n",
    "    def update(self,\n",
    "               result,\n",
    "               method_name='computed',\n",
    "               ):\n",
    "        \"\"\"\n",
    "        Store the result of the curvature computation by passing the computed curvature of the center (first) point.\n",
    "        \"\"\"\n",
    "        self.DS[self.idx].results[method_name] = result\n",
    "\n",
    "    def compute_metrics(self):\n",
    "        self._aggregate_labels()\n",
    "        metrics = self._get_metrics()\n",
    "        self.metric_table = {}\n",
    "        for metric in metrics:\n",
    "            self.metric_table[metric.__name__] = {}\n",
    "            for method_name in self.method_names:\n",
    "                self.metric_table[metric.__name__][method_name] = self.compute(metric=metric, method_name=method_name)\n",
    "        self.metric_table = pd.DataFrame(self.metric_table)\n",
    "            \n",
    "    def compute(self, metric, method_name):\n",
    "        # Overwrite this class with your logic. It implements the computation of a single metric for a single method\n",
    "        return metric(self.labels[method_name], self.labels['ground truth'])\n",
    "    \n",
    "\n",
    "    def _aggregate_labels(self):\n",
    "        # returns a dictionary whose keys are method names, paired with a list of each of the results given by the metrics.\n",
    "        # Just a more convenient data format for comparing method outputs.\n",
    "        self.method_names = list(self.DS[0].results.keys())\n",
    "        self.labels = {}\n",
    "        for m in self.method_names:\n",
    "            self.labels[m] = [self.DS[i].results[m] for i in range(self.__len__())]\n",
    "\n",
    "    def plot(self, title = None):\n",
    "        if title is None: title = f\"In dimension {self.dimension}\"\n",
    "        # for each computed method on this dataset, we plot the histogram of saddles vs spheres\n",
    "        self._aggregate_labels()\n",
    "        # get the idxs for each type of dataset\n",
    "        dataset_names = [self.DS.data_vars[i].attrs['name'] for i in range(len(self.DS))]\n",
    "        unique_names = list(set(dataset_names))\n",
    "        idxs_by_name = {n: [i for i, name in enumerate(dataset_names) if name == n] for n in unique_names}        \n",
    "        for m in self.method_names: \n",
    "            if m != 'ks' and m != 'name':\n",
    "                for dname in unique_names:\n",
    "                    plt.hist(self.labels[m][idxs_by_name[dname]], bins=50, label = dname, edgecolor='none', linewidth=5)\n",
    "                plt.legend()\n",
    "                plt.xlabel(m)\n",
    "                plt.title(title)\n",
    "                plt.show()\n",
    "\n",
    "    def table(self):\n",
    "        self.compute_metrics()\n",
    "        return self.metric_table\n",
    "\n",
    "    def _get_metrics(self):\n",
    "        tagged_functions = []\n",
    "        for name, member in inspect.getmembers(self, predicate=inspect.ismethod):\n",
    "            if hasattr(member, 'tag') and getattr(member, 'tag') == 'metric':\n",
    "                tagged_functions.append(member)\n",
    "        return tagged_functions\n",
    "\n",
    "    @metric\n",
    "    def pearson_r(self, a, b):\n",
    "        return scipy.stats.pearsonr(a,b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "Red\n"
     ]
    }
   ],
   "source": [
    "dls = [3,1,4,1,5,9,2,6,5]\n",
    "class Wrapper:\n",
    "    def __init__(self, obj, **kwargs):\n",
    "        self.obj = obj\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "# Example usage with a list of strings (but can be any objects)\n",
    "original_objects = [\"Apple\", \"Banana\"]\n",
    "\n",
    "wrapped_objects = [\n",
    "    Wrapper(obj, color=\"Red\", weight=150) for obj in original_objects\n",
    "]\n",
    "\n",
    "# Accessing attributes\n",
    "print(wrapped_objects[0].obj)  # Output: Apple\n",
    "print(wrapped_objects[0].color)  # Output: Red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Red'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_objects[1].__dict__['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
