{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a Autoencoders\n",
    "A significant portion of the training machinery from the original repo can be subsumed by the unwitting use of a *framework!* Yes, frameworks - factory factory factories and all, we love them.\n",
    "The only nonstandard bit from the bespoke class above which affects the downstream geometry is using `immersion` as an alias for `decoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class DerrickTheAutoencoder(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Derrick doesn't have many friends, so he spends his time encoding data into the specified latent dimension using tanh activations.\n",
    "    Fortunately, he's friendly with PyTorch Lightning; he gets struck occasionally, but that's made up by his lickety-split training.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, intrinsic_dimension, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.intrinsic_dimension = intrinsic_dimension\n",
    "        self.lr = learning_rate\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim, out_features=256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=64, out_features=intrinsic_dimension)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=intrinsic_dimension, out_features=64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=256, out_features=input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def immersion(self, x):\n",
    "        \"\"\"The immersion defined by the autoencoder (or more precisely, the decoder).\"\"\"\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x_hat = self(x)\n",
    "        loss = nn.MSELoss()(x_hat, x)\n",
    "        self.log('train_loss', loss, prog_bar=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x_hat = self(x)\n",
    "        loss = nn.MSELoss()(x_hat, x)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        x_hat = self(x)\n",
    "        loss = nn.MSELoss()(x_hat, x)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Matching Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class DistanceMatchingAutoencoder(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    What if you already know what your latent space should look like, but want to learn a differentiable mapping into it?\n",
    "    Enter the DistanceMatchingAutoencoder, or DISMA for short. In addition to a mean squared error loss, it also penalizes the difference between the pairwise distances of the embedding and the supplied ground truth.\n",
    "\n",
    "    Each minibatch from the dataloader is assumed to have the following keys:\n",
    "    - `x`: the input data\n",
    "    - `d`: the ground truth pairwise distances\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, intrinsic_dim, learning_rate=1e-3, reconstruction_weight=1, distance_weight=1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.intrinsic_dimension = intrinsic_dim\n",
    "        self.lr = learning_rate\n",
    "        self.reconstruction_weight = reconstruction_weight\n",
    "        self.distance_weight = distance_weight\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim, out_features=256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=64, out_features=intrinsic_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=intrinsic_dim, out_features=64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=256, out_features=input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def immersion(self, x):\n",
    "        \"\"\"The immersion defined by the autoencoder (or more precisely, the decoder).\"\"\"\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def distance_loss(self, x_embedded, ground_truth_distances):\n",
    "        embedding_distances = torch.cdist(x_embedded, x_embedded)\n",
    "        prepared_embedded = torch.log(embedding_distances + torch.eye(embedding_distances.shape[0], device=x_embedded.device) + 1e-10)        \n",
    "        prepared_truth = torch.log(ground_truth_distances + torch.eye(ground_truth_distances.shape[0], device=x_embedded.device) + 1e-10)\n",
    "        return nn.MSELoss()(prepared_embedded, prepared_truth)\n",
    "\n",
    "    def step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        d = batch['d']\n",
    "        x_embedded = self.encoder(x)\n",
    "        x_hat = self.decoder(x_embedded)\n",
    "        recon_loss = nn.MSELoss()(x_hat, x)\n",
    "        dist_loss = self.distance_loss(x_embedded, d)\n",
    "        loss = self.reconstruction_weight * recon_loss + self.distance_weight * dist_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch, batch_idx)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch, batch_idx)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch, batch_idx)\n",
    "        self.log('test_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coordinate-wise distance matching autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CoordinatewiseDistanceMatchingAutoencoder(DistanceMatchingAutoencoder):\n",
    "    def __init__(self, input_dim, intrinsic_dim, learning_rate=0.001, reconstruction_weight=1, distance_weight=1):\n",
    "        super().__init__(input_dim, intrinsic_dim, learning_rate, reconstruction_weight, distance_weight)\n",
    "    \n",
    "    def distance_loss(self, x_embedded, phate_coords):\n",
    "        phate_coords_permuted = phate_coords.unsqueeze(0).permute(2, 1, 0)\n",
    "        ground_truth_distances = torch.cdist(phate_coords_permuted, phate_coords_permuted)\n",
    "        x_embedded_permuted = x_embedded.unsqueeze(0).permute(2, 1, 0)\n",
    "        embedding_distances = torch.cdist(x_embedded_permuted, x_embedded_permuted)\n",
    "        prepared_embedded = torch.log(embedding_distances + torch.eye(embedding_distances.shape[1], device=x_embedded.device) + 1e-10)        \n",
    "        prepared_truth = torch.log(ground_truth_distances + torch.eye(ground_truth_distances.shape[1], device=x_embedded.device) + 1e-10)\n",
    "        return nn.MSELoss()(prepared_embedded, prepared_truth)\n",
    "    \n",
    "    def step(self, batch, batch_idx):\n",
    "        x, phate_coords = batch['x'], batch['p']\n",
    "        x_embedded = self.encoder(x)\n",
    "        x_hat = self.decoder(x_embedded)\n",
    "        reconstruction_loss = nn.MSELoss()(x_hat, x)\n",
    "        distance_loss = self.distance_loss(x_embedded, phate_coords)\n",
    "        loss = self.reconstruction_weight * reconstruction_loss + self.distance_weight * distance_loss\n",
    "        # self.log('train_loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class FlexibleDMAutoencoder(DistanceMatchingAutoencoder):\n",
    "    def __init__(self, input_dim, intrinsic_dim, learning_rate=0.001, reconstruction_weight=1, distance_weight=1, coordinatewise=False, pretrain=False):\n",
    "        super().__init__(input_dim, intrinsic_dim, learning_rate, reconstruction_weight, distance_weight)\n",
    "        self.coordinatewise = coordinatewise\n",
    "        self.pretrain = pretrain\n",
    "    \n",
    "    def distance_loss_coorw(self, x_embedded, phate_coords):\n",
    "        phate_coords_permuted = phate_coords.unsqueeze(0).permute(2, 1, 0)\n",
    "        ground_truth_distances = torch.cdist(phate_coords_permuted, phate_coords_permuted)\n",
    "        x_embedded_permuted = x_embedded.unsqueeze(0).permute(2, 1, 0)\n",
    "        embedding_distances = torch.cdist(x_embedded_permuted, x_embedded_permuted)\n",
    "        prepared_embedded = torch.log(embedding_distances + torch.eye(embedding_distances.shape[1], device=x_embedded.device) + 1e-10)        \n",
    "        prepared_truth = torch.log(ground_truth_distances + torch.eye(ground_truth_distances.shape[1], device=x_embedded.device) + 1e-10)\n",
    "        return nn.MSELoss()(prepared_embedded, prepared_truth)\n",
    "    \n",
    "    def pretrain_loss(self, x_embedded, mds_coords):\n",
    "        return nn.MSELoss()(x_embedded, mds_coords)\n",
    "        \n",
    "    def step(self, batch, batch_idx):\n",
    "        x = batch['x']\n",
    "        x_embedded = self.encoder(x)\n",
    "        x_hat = self.decoder(x_embedded)\n",
    "        recon_loss = nn.MSELoss()(x_hat, x)\n",
    "        if self.pretrain:\n",
    "            mds_coords = batch['m']\n",
    "            dist_loss = self.pretrain_loss(x_embedded, mds_coords)\n",
    "        elif self.coordinatewise:\n",
    "            phate_coords = batch['p']\n",
    "            dist_loss = self.distance_loss_coorw(x_embedded, phate_coords)\n",
    "        else:\n",
    "            d = batch['d']\n",
    "            dist_loss = self.distance_loss(x_embedded, d)\n",
    "        loss = self.reconstruction_weight * recon_loss + self.distance_weight * dist_loss\n",
    "        return loss\n",
    "\n",
    "    def set_pretrain(self, distance_weight, reconstruction_weight):\n",
    "        self.pretrain = True\n",
    "        self.distance_weight = distance_weight\n",
    "        self.reconstruction_weight = reconstruction_weight\n",
    "\n",
    "    def set_finetune(self, distance_weight, reconstruction_weight, coordinatewise):\n",
    "        self.pretrain = False\n",
    "        self.distance_weight = distance_weight\n",
    "        self.reconstruction_weight = reconstruction_weight\n",
    "        self.coordinatewise = coordinatewise\n",
    "    \n",
    "    def set_coordinatewise(self):\n",
    "        self.coordinatewise = True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determinant Regularized Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp autoencoders\n",
    "LOWER_EPSILON = 1e-20\n",
    "BIGGER_LOWER_EPSILON = 1e-12\n",
    "BIGGEST_LOWER_EPSILON = 1e-10\n",
    "UPPER_EPSILON = 1e20\n",
    "SMALLER_UPPER_EPSILON = 1e12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\"\"\"\n",
    "THIS FILE WAS TAKEN FROM https://github.com/BorgwardtLab/topological-autoencoders\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Base class for autoencoder models.\"\"\"\n",
    "\n",
    "import abc\n",
    "from typing import Dict, Tuple\n",
    "import torch.nn as nn\n",
    "class AutoencoderModel(nn.Module, metaclass=abc.ABCMeta):\n",
    "    \"\"\"Abstract base class for autoencoders.\"\"\"\n",
    "\n",
    "    # pylint: disable=W0221\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, x) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"Compute loss for model.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor with data\n",
    "\n",
    "        Returns:\n",
    "            Tuple[loss, dict(loss_component_name -> loss_component)]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def encode(self, x):\n",
    "        \"\"\"Compute latent representation.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def decode(self, z):\n",
    "        \"\"\"Compute reconstruction.\"\"\"\n",
    "\n",
    "    def immersion(self, x):\n",
    "        \"\"\"The immersion defined by the autoencoder (or more precisely, the decoder).\"\"\"\n",
    "        return self.decoder(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\"\"\"\n",
    "THIS FILE WAS PARTLY TAKEN FROM https://github.com/BorgwardtLab/topological-autoencoders\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Submodules used by models.\"\"\"\n",
    "\n",
    "\n",
    "# Hush the linter: Warning W0221 corresponds to a mismatch between parent class\n",
    "# method signature and the child class\n",
    "# pylint: disable=W0221\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n",
    "\n",
    "class Print(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(self.name, x.size())\n",
    "        return x\n",
    "\n",
    "\n",
    "class BoxAutoEncoder(AutoencoderModel):\n",
    "    \"\"\"100-100-100-2-100-100-100.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dims=(1, 28, 28), **kwargs):\n",
    "        super().__init__()\n",
    "        self.latent_dim = 2\n",
    "        n_input_dims = np.prod(input_dims)\n",
    "        self.input_dim = n_input_dims.item()\n",
    "        self.input_dims = input_dims\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # View((-1, n_input_dims)),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.input_dim, out_features=100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=100, out_features=self.latent_dim),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.latent_dim, out_features=100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(in_features=100, out_features=self.input_dim),\n",
    "            # View((-1,) + tuple(input_dims)),\n",
    "        )\n",
    "\n",
    "        self.reconst_error = nn.MSELoss()\n",
    "\n",
    "        self.register_hook()\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Compute latent representation using convolutional autoencoder.\"\"\"\n",
    "        x = x.view((-1, self.input_dim))\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Compute reconstruction using convolutional autoencoder.\"\"\"\n",
    "        z = self.decoder(z)\n",
    "        z = z.view((-1, *self.input_dims))\n",
    "        return z\n",
    "\n",
    "    def forward_(self, x):\n",
    "        x = x.view((-1, self.input_dim))\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        z = self.decoder(x)\n",
    "        z = z.view((-1, *self.input_dims))\n",
    "\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply autoencoder to batch of input images.\n",
    "\n",
    "        Args:\n",
    "            x: Batch of images with shape [bs x channels x n_row x n_col]\n",
    "\n",
    "        Returns:\n",
    "            tuple(reconstruction_error, dict(other errors))\n",
    "\n",
    "        \"\"\"\n",
    "        latent = self.encode(x)\n",
    "        x_reconst = self.decode(latent)\n",
    "        reconst_error = self.reconst_error(x, x_reconst)\n",
    "        return reconst_error, {'reconstruction_error': reconst_error}\n",
    "\n",
    "    def register_hook(self):\n",
    "        self.encoder.register_forward_hook(self.get_activation())\n",
    "\n",
    "    def get_activation(self):\n",
    "        \"\"\"\n",
    "        :return: activations at layer model\n",
    "        \"\"\"\n",
    "\n",
    "        def hook(model, input, output):\n",
    "            self.latent_activations = output\n",
    "            self.latent_activations_detached = output.detach()\n",
    "\n",
    "        return hook\n",
    "\n",
    "    def load(self, path):\n",
    "        dict = torch.load(path)\n",
    "\n",
    "        # edit keys, since they are created from higher TopoAE class\n",
    "        new_dict = OrderedDict([])\n",
    "\n",
    "        for key in dict.keys():\n",
    "            arr = key.split(\".\")\n",
    "            if len(arr) == 1 or arr[-1] not in [\"weight\", \"bias\"]:\n",
    "                continue\n",
    "                # del dict[key]\n",
    "            # elif arr[-1] not in [\"weight\", \"bias\"]:\n",
    "            #    del dict[key]\n",
    "            else:\n",
    "                if arr[0] == \"autoencoder\":\n",
    "                    new_key = \".\".join(arr[1:])\n",
    "                else:\n",
    "                    new_key = key\n",
    "\n",
    "                new_dict.update({new_key: dict[key]})\n",
    "\n",
    "                # dict = OrderedDict([(new_key, v) if k == key else (k, v) for k, v in d.items()])\n",
    "                # new_dict.update({new_key: dict[key]})\n",
    "\n",
    "        # self.load_state_dict(torch.load(path))\n",
    "        self.load_state_dict(new_dict)\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "\n",
    "class ConvolutionalAutoEncoder(AutoencoderModel):\n",
    "    \"\"\"Convolutional autoencoder for MNIST and FashionMNIST\"\"\"\n",
    "\n",
    "    def __init__(self, input_dims=(1, 28, 28), **kwargs):\n",
    "        super().__init__()\n",
    "        self.latent_dim = 2\n",
    "        n_input_dims = np.prod(input_dims)\n",
    "        self.input_dim = n_input_dims.item()\n",
    "        self.input_dims = input_dims\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # b, 1, 28, 28\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(2, stride=1),  # b, 8, 2, 2\n",
    "            nn.Conv2d(8, 2, 2, stride=1, padding=0),  # b, 2, 1, 1\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2, 8, 2, stride=1),  # b, 8, 2, 2\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # b, 1, 28, 28\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.reconst_error = nn.MSELoss()\n",
    "\n",
    "        self.register_hook()\n",
    "\n",
    "    def immersion(self, x):\n",
    "        return self.decoder(x).view(-1, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Compute latent representation using convolutional autoencoder.\"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Compute reconstruction using convolutional autoencoder.\"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward_(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z = self.decoder(x)\n",
    "\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply autoencoder to batch of input images.\n",
    "\n",
    "        Args:\n",
    "            x: Batch of images with shape [bs x channels x n_row x n_col]\n",
    "\n",
    "        Returns:\n",
    "            tuple(reconstruction_error, dict(other errors))\n",
    "\n",
    "        \"\"\"\n",
    "        latent = self.encode(x)\n",
    "        x_reconst = self.decode(latent)\n",
    "        reconst_error = self.reconst_error(x, x_reconst)\n",
    "        return reconst_error, {'reconstruction_error': reconst_error}\n",
    "\n",
    "    def register_hook(self):\n",
    "        self.encoder.register_forward_hook(self.get_activation())\n",
    "\n",
    "    def get_activation(self):\n",
    "        \"\"\"\n",
    "        :return: activations at layer model\n",
    "        \"\"\"\n",
    "\n",
    "        def hook(model, input, output):\n",
    "            self.latent_activations = output\n",
    "            self.latent_activations_detached = output.detach()\n",
    "\n",
    "        return hook\n",
    "\n",
    "    def load(self, path):\n",
    "        dict = torch.load(path)\n",
    "\n",
    "        # edit keys, since they are created from higher TopoAE class\n",
    "        new_dict = OrderedDict([])\n",
    "\n",
    "        for key in dict.keys():\n",
    "            arr = key.split(\".\")\n",
    "            if len(arr) == 1 or arr[-1] not in [\"weight\", \"bias\"]:\n",
    "                continue\n",
    "                # del dict[key]\n",
    "            # elif arr[-1] not in [\"weight\", \"bias\"]:\n",
    "            #    del dict[key]\n",
    "            else:\n",
    "                if arr[0] == \"autoencoder\":\n",
    "                    new_key = \".\".join(arr[1:])\n",
    "                else:\n",
    "                    new_key = key\n",
    "\n",
    "                new_dict.update({new_key: dict[key]})\n",
    "\n",
    "                # dict = OrderedDict([(new_key, v) if k == key else (k, v) for k, v in d.items()])\n",
    "                # new_dict.update({new_key: dict[key]})\n",
    "\n",
    "        # self.load_state_dict(torch.load(path))\n",
    "        self.load_state_dict(new_dict)\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "\n",
    "class LinearAE(AutoencoderModel):\n",
    "    \"\"\"input dim - 2 - input dim.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dims=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.input_dims = input_dims\n",
    "        n_input_dims = np.prod(input_dims)\n",
    "        self.encoder = nn.Sequential(\n",
    "            View((-1, n_input_dims)),\n",
    "            nn.Linear(n_input_dims, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, n_input_dims),\n",
    "            View((-1,) + tuple(input_dims)),\n",
    "        )\n",
    "        self.reconst_error = nn.MSELoss()\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Compute latent representation using convolutional autoencoder.\"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Compute reconstruction using convolutional autoencoder.\"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply autoencoder to batch of input images.\n",
    "\n",
    "        Args:\n",
    "            x: Batch of images with shape [bs x channels x n_row x n_col]\n",
    "\n",
    "        Returns:\n",
    "            tuple(reconstruction_error, dict(other errors))\n",
    "\n",
    "        \"\"\"\n",
    "        latent = self.encode(x)\n",
    "        x_reconst = self.decode(latent)\n",
    "        reconst_error = self.reconst_error(x, x_reconst)\n",
    "        return reconst_error, {'reconstruction_error': reconst_error}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Determinant Regularizer\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "from autometric.manifolds import RiemannianManifold\n",
    "from autometric.metrics import PullbackMetric\n",
    "from autometric.connections import LeviCivitaConnection\n",
    "LOWER_EPSILON = 1e-12\n",
    "UPPER_EPSILON = 1e-3\n",
    "\n",
    "\n",
    "class Loss:\n",
    "    \"\"\"\n",
    "    A Basis class for custom loss functions\n",
    "    \"\"\"\n",
    "    def __init__(self, model=None):\n",
    "        # a manifold object\n",
    "        self.model = model\n",
    "        pbm = PullbackMetric(2, model.immersion)\n",
    "        lcc = LeviCivitaConnection(2, pbm)\n",
    "        self.manifold = RiemannianManifold(2, (1, 1), metric=pbm, connection=lcc)\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_points(latent_activations=None, outputs=None, num_samples=1):\n",
    "        \"\"\"\n",
    "        Randomly sample points from batch in latent space and map it to the output space\n",
    "        :param outputs: batch in output space\n",
    "        :param latent_activations: batch in latent space\n",
    "        :param num_samples: number of samples to take\n",
    "        :return: (points in latent space, points in output space)\n",
    "        \"\"\"\n",
    "\n",
    "        # randomly sample the points\n",
    "        rand_choice = torch.randperm(latent_activations.shape[0])[:num_samples]\n",
    "\n",
    "        pimg_origin = latent_activations[rand_choice, :]\n",
    "\n",
    "        if outputs is None:\n",
    "            return pimg_origin\n",
    "\n",
    "        img_origin = outputs[rand_choice, :]\n",
    "\n",
    "        return img_origin, pimg_origin\n",
    "\n",
    "\n",
    "class DeterminantLoss(Loss):\n",
    "    def __call__(self, epoch=0, *args, **kwargs):\n",
    "        \"\"\"\n",
    "            Our Determinant Regularizer\n",
    "        Args:\n",
    "            epoch: current epoch\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # here you can control whether the regularizer should only be switched on after a certain epoch\n",
    "        if epoch >= 0:\n",
    "            loss_det = self.determinant_loss()\n",
    "        else:\n",
    "            loss_det = torch.tensor([0.], device=device, requires_grad=True)\n",
    "\n",
    "        return loss_det\n",
    "\n",
    "    def determinant_loss(self):\n",
    "        \"\"\"\n",
    "        Calculate the actual loss\n",
    "        Returns:\n",
    "            The determinant loss\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate the logarithm of the generalized jacobian determinant\n",
    "        log_dets = self.manifold.metric_logdet(base_point=self.model.latent_activations)\n",
    "\n",
    "        # replace nan values with a small number\n",
    "        #EPSILON = 1e-9\n",
    "        #torch.nan_to_num(log_dets, nan=EPSILON, posinf=EPSILON, neginf=EPSILON)\n",
    "        torch.nan_to_num(log_dets, nan=1., posinf=1., neginf=1.)\n",
    "\n",
    "        # calculate the variance of the logarithm of the generalized jacobian determinant\n",
    "        raw_loss = torch.var(log_dets)\n",
    "\n",
    "        return raw_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\"\"\"\n",
    "THIS FILE WAS PARTLY TAKEN FROM https://github.com/BorgwardtLab/topological-autoencoders\n",
    "\"\"\"\n",
    "\"\"\"Vanilla models.\"\"\"\n",
    "\n",
    "class VanillaAutoencoderModel(AutoencoderModel):\n",
    "    def __init__(self, autoencoder_model='ConvolutionalAutoencoder',\n",
    "                 ae_kwargs=None):\n",
    "        super().__init__()\n",
    "\n",
    "        ae_kwargs = ae_kwargs if ae_kwargs else {}\n",
    "        self.autoencoder = getattr(submodules, autoencoder_model)(**ae_kwargs)\n",
    "\n",
    "        self.with_geom_loss = False\n",
    "\n",
    "        if self.with_geom_loss:\n",
    "            self.determinant_criterion = DeterminantLoss(model=self.autoencoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use reconstruction loss of autoencoder\n",
    "\n",
    "        if self.with_geom_loss:\n",
    "            ae_loss, ae_loss_comp = self.autoencoder(x)\n",
    "\n",
    "            det_loss = self.determinant_criterion().detach_()\n",
    "\n",
    "            loss = ae_loss\n",
    "            loss_components = {\n",
    "                'loss.autoencoder': ae_loss,\n",
    "                'loss.geom_error': det_loss\n",
    "            }\n",
    "\n",
    "            loss_components.update(ae_loss_comp)\n",
    "            return (\n",
    "                loss,\n",
    "                loss_components\n",
    "            )\n",
    "        else:\n",
    "            return self.autoencoder(x)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.autoencoder.encode(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.autoencoder.decode(z)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
